VideoEncoder: {
    "patch_size": [1,8,8],
    "embed_dim": 128,
    "depths":[2, 2, 14, 2, 2, 2],
    "downsample_stages":[0, 1, 4],
    "stages":[0, 1, 2, 2, 2, 3],
    "num_heads":[4, 8, 16, 16, 16, 32],
    "window_size":[[2,3,5],[4,3,5],[8,3,5],[16,3,5],[16,3,5],[32,3,5]], #time, h, w
    "patch_norm": True,
    "local_window": 8
}



bert_config: "src/configs/bert_large_config.json"
stage: 2
type_vocab_size: 8
num_local_layers: 8
stage1_layers: 12
bert_frozen_stage: -1
final_num_patches: 6

qa_type: 'classification'

WEIGHTS:
    model_weight: 'project/lfvila/pretrained/lfvila_stage2.bin'
    stage1_model_weight: ''
    bert_weight: 'project/lfvila/pretrained/bert-large-uncased/pytorch_model.bin'
    swin_weight: 'project/lfvila/pretrained/swin/swin_base_patch4_window12_384_22k.pth'
    pretrained_2d: True

DATA:
    BATCH_SIZE_per_gpu: 12
    NUM_WORKERS: 8
    PIN_MEMORY: True

    sample_frame: 32
    sample_clip: 4
    input_res: [192, 320]
    center_crop: 200

    max_num_subtitle: 4

    classification_labels: 2

    DATASET_train: {
            'name': 'QADataset-train',
            'type': 'ViolinDataset',
            'metadata_dir': 'datasets/lfvila_data/task/violin/violin_train.jsonl',
            'video_path': 'datasets/violin/violin_video'
        }

    DATASET_val: [{
            'name': 'QADataset-val',
            'type': 'ViolinDataset',
            'metadata_dir': 'datasets/lfvila_data/task/violin/violin_test.jsonl',
            'video_path': 'datasets/violin/violin_video'
        }]


TRAINING:
    EPOCHS: 100
    WARMUP_EPOCHS: 10
    WARMUP_LR: 0.
    LR_SCHEDULER: {
        'NAME': 'linear',
        'DECAY_EPOCHS': 10,
        }


    use_mlm: false

    weight_decay: 0.1

    save_dir: "project/lfvila/lfvila_save/violin"
    checkpoint_step: 10000
    save_step: 5000
    print_step: 100
    eval_step: 500

deepspeed_config: {
    "train_micro_batch_size_per_gpu": 12,
    "gradient_accumulation_steps": 1,
    "steps_per_print": 500,


    "zero_optimization": {
      "stage": 2,
      "allgather_partitions": true,
      "allgather_bucket_size": 5.0e+8,
      "overlap_comm": false,
      "reduce_scatter": true,
      "reduce_bucket_size": 5.0e+8,
      "contiguous_gradients" : false,
      "stage3_gather_fp16_weights_on_model_save": true
    },

    "fp16": {
      "enabled": true,
      "loss_scale": 0,
      "loss_scale_window": 1000,
      "initial_scale_power": 32,
      "hysteresis": 2,
      "min_loss_scale": 1
  },

    "optimizer": {
        "type": "AdamW",
        "params": {
        "lr": 5.0e-5,
        "betas": [0.9, 0.98],
        "eps": 1.0e-8,
        "weight_decay": 5.0e-2
        }
    },


    "sparse_attention": {
      "mode": "fixed",
      "block": 32,
      "different_layout_per_head": true,
      "num_local_blocks": 16,
      "num_global_blocks": 1,
      "attention": "bidirectional",
      "horizontal_global_attention": true,
      "num_different_global_patterns": 4
    }
}

  


  
